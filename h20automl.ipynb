{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['air_store_info.csv', 'hpg_store_info.csv', 'sample_submission.csv', 'date_info.csv', 'hpg_reserve.csv', 'air_visit_data.csv', 'air_reserve.csv', 'store_id_relation.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d63ad31e45df48617331a3d842ec50abe191e88"
      },
      "cell_type": "code",
      "source": "import glob, re\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nfrom datetime import datetime\nfrom xgboost import XGBRegressor",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n  DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import h2o\nfrom h2o.automl import H2OAutoML",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b32e17d6cd53d2e5caba4eaccf823097492eb391"
      },
      "cell_type": "code",
      "source": "h2o.init()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\nAttempting to start a local H2O server...\n  Java Version: java version \"1.8.0_161\"; Java(TM) SE Runtime Environment (build 1.8.0_161-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)\n  Starting server from /opt/conda/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n  Ice root: /tmp/tmpk81zy4_f\n  JVM stdout: /tmp/tmpk81zy4_f/h2o_unknownUser_started_from_python.out\n  JVM stderr: /tmp/tmpk81zy4_f/h2o_unknownUser_started_from_python.err\n  Server is running at http://127.0.0.1:54321\nConnecting to H2O server at http://127.0.0.1:54321... successful.\nWarning: Your H2O cluster version is too old (4 months and 27 days)! Please download and install the latest version from http://h2o.ai/download/\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "--------------------------  ----------------------------------------\nH2O cluster uptime:         23 secs\nH2O cluster timezone:       Etc/UTC\nH2O data parsing timezone:  UTC\nH2O cluster version:        3.18.0.4\nH2O cluster version age:    4 months and 27 days !!!\nH2O cluster name:           H2O_from_python_unknownUser_hk0dko\nH2O cluster total nodes:    1\nH2O cluster free memory:    3.484 Gb\nH2O cluster total cores:    2\nH2O cluster allowed cores:  2\nH2O cluster status:         accepting new members, healthy\nH2O connection url:         http://127.0.0.1:54321\nH2O connection proxy:\nH2O internal security:      False\nH2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\nPython version:             3.6.4 final\n--------------------------  ----------------------------------------",
            "text/html": "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n<td>23 secs</td></tr>\n<tr><td>H2O cluster timezone:</td>\n<td>Etc/UTC</td></tr>\n<tr><td>H2O data parsing timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O cluster version:</td>\n<td>3.18.0.4</td></tr>\n<tr><td>H2O cluster version age:</td>\n<td>4 months and 27 days !!!</td></tr>\n<tr><td>H2O cluster name:</td>\n<td>H2O_from_python_unknownUser_hk0dko</td></tr>\n<tr><td>H2O cluster total nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O cluster free memory:</td>\n<td>3.484 Gb</td></tr>\n<tr><td>H2O cluster total cores:</td>\n<td>2</td></tr>\n<tr><td>H2O cluster allowed cores:</td>\n<td>2</td></tr>\n<tr><td>H2O cluster status:</td>\n<td>accepting new members, healthy</td></tr>\n<tr><td>H2O connection url:</td>\n<td>http://127.0.0.1:54321</td></tr>\n<tr><td>H2O connection proxy:</td>\n<td>None</td></tr>\n<tr><td>H2O internal security:</td>\n<td>False</td></tr>\n<tr><td>H2O API Extensions:</td>\n<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n<tr><td>Python version:</td>\n<td>3.6.4 final</td></tr></table></div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84c1dc8964f045828845c468dee33198110b051e"
      },
      "cell_type": "code",
      "source": "data = {\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/air_reserve.csv'),\n    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n    'id': pd.read_csv('../input/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n    }\n\ndata['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n\nfor df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n\ndata['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n\nunique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n\n#sure it can be compressed...\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n\nstores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n# NEW FEATURES FROM Georgii Vyshnia\nstores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\nstores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\nlbl = preprocessing.LabelEncoder()\nfor i in range(4):\n    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n    stores['air_area_name' +str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n\ntrain = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \ntest = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n\nfor df in ['ar','hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n\ntrain['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n\ntrain['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\ntrain['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\ntrain['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\ntest['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\ntest['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\ntest['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n\n# NEW FEATURES FROM JMBULL\ntrain['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntrain['var_max_lat'] = train['latitude'].max() - train['latitude']\ntrain['var_max_long'] = train['longitude'].max() - train['longitude']\ntest['var_max_lat'] = test['latitude'].max() - test['latitude']\ntest['var_max_long'] = test['longitude'].max() - test['longitude']\n\n# NEW FEATURES FROM Georgii Vyshnia\ntrain['lon_plus_lat'] = train['longitude'] + train['latitude'] \ntest['lon_plus_lat'] = test['longitude'] + test['latitude']\n\nlbl = preprocessing.LabelEncoder()\ntrain['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\ntest['air_store_id2'] = lbl.transform(test['air_store_id'])\n\ntrain = train.fillna(-999)\ntest = test.fillna(-999)\n\ntrain['visitors'] = np.log1p(train['visitors'].values)\n\nprint('Pre-processing done!')",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Pre-processing done!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ced8ff15e9a180fbca4ef00fdc036e56b4590ad7"
      },
      "cell_type": "code",
      "source": "htrain = h2o.H2OFrame(train)\nhtest = h2o.H2OFrame(test)\n\nhtrain.drop(['id', 'air_store_id', 'visit_date'])\nhtest.drop(['id', 'air_store_id', 'visit_date'])\n\nx =htrain.columns\ny ='visitors'\nx.remove(y)\n\ndef RMSLE(y_, pred):\n    return metrics.mean_squared_error(y_, pred)**0.5",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Parse progress: |█████████████████████████████████████████████████████████| 100%\nParse progress: |█████████████████████████████████████████████████████████| 100%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b20eec043f9c539678005bbae788bf5b0aa5301"
      },
      "cell_type": "code",
      "source": "print('Starting h2o autoML model!')  \n\naml = H2OAutoML(max_runtime_secs = 3350)\naml.train(x=x, y =y, training_frame=htrain, leaderboard_frame = htest)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Starting h2o autoML model!\nAutoML progress: |████████████████████████████████████████████████████████| 100%\nParse progress: |█████████████████████████████████████████████████████████| 100%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab300a6c126deab5fad29469c371e22a23a749ef"
      },
      "cell_type": "code",
      "source": "print('Generate predictions...')\nhtrain.drop(['visitors'])\npreds = aml.leader.predict(htrain)\npreds = preds.as_data_frame()\nprint('RMSLE H2O automl leader: ', RMSLE(train['visitors'].values, preds))",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Generate predictions...\nstackedensemble prediction progress: |████████████████████████████████████| 100%\nRMSLE H2O automl leader:  0.44906017526287556\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2a4c43eaa4c504de037058c7a0657d211398406"
      },
      "cell_type": "code",
      "source": "preds = aml.leader.predict(htest)\npreds = preds.as_data_frame()\n\ntest['visitors'] = preds\ntest['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\nsub1 = test[['id','visitors']].copy()\ndel train; del data; del htrain; del htest;",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "5f28c23cd6becc7d1cf73910aac0a5668116979e"
      },
      "cell_type": "code",
      "source": "# from hklee\n# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\ndfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n    pd.read_csv(fn)for fn in glob.glob('../input/*.csv')}\n\nfor k, v in dfs.items(): locals()[k] = v\n\nwkend_holidays = date_info.apply(\n    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\ndate_info.loc[wkend_holidays, 'holiday_flg'] = 0\ndate_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n\nvisit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\nvisit_data.drop('calendar_date', axis=1, inplace=True)\nvisit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n\nwmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\nvisitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\nvisitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n\nsample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\nsample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\nsample_submission.drop('visitors', axis=1, inplace=True)\nsample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\nsample_submission = sample_submission.merge(visitors, on=[\n    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n    how='left')['visitors_y'].values\n\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n    on='air_store_id', how='left')['visitors_y'].values\n\nsample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\nsub2 = sample_submission[['id', 'visitors']].copy()\nsub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n\nsub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\nsub_merge[['id', 'visitors']].to_csv('submission.csv', index=False)\n\nprint('Leaderboard : ', aml.leaderboard)\n\nprint(' H2O automl leader performace : ', aml.leader)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a5d9e9f2a07b61d4bf468bea0ce1acda1039ed93"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "43084c66a5f62079f8d9dd0168a45a31b9511436"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}